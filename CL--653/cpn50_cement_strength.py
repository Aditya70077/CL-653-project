# -*- coding: utf-8 -*-
"""cpn50-cement-strength.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IgzwgP7XW22Aki_efBwpwDMgC91vVpjN

# CPN50 Cement Strength

## Uploading libraries
"""

###
import numpy as np
import pandas as pd
###
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
###

import seaborn as sns
import matplotlib.pyplot as plt

from numpy import mean
from numpy import std
from scipy.stats import skew, kurtosis
!pip install catboost

from sklearn.dummy import DummyRegressor
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from catboost import CatBoostRegressor

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

"""## Uploading dataset"""

path = "/content/CPN50_cement_composition_strength.csv"
df = pd.read_csv(path)
df.drop(columns = "Unnamed: 0", inplace = True)
print(df.shape)
df.head(3)

features = df.columns.tolist()
targets = ["r1_iram1622", "r2_iram1622", "r3_iram1622", "r7_iram1622", "r28_iram1622", "r91_iram1622"]

for feat in targets:
    features.remove(feat)
print(features)

"""## Removing non value features"""

## Identifying featurs with more than 25% of nanÂ´s and dropping

feats_2_drop = []
for feat in features:
    if df[feat].isna().sum() > df.shape[0]/25:
        feats_2_drop.append(feat)

for feat in feats_2_drop:
    features.remove(feat)
print(features)

feats_2_drop2 = ["ric", "rie", "pl", "em", "nat", "date"]
for feat in feats_2_drop2:
    features.remove(feat)
print(features)

targets_2_drop = ["r3_iram1622", "r91_iram1622"]
for feat in targets_2_drop:
    targets.remove(feat)
print(targets)

df[features].info()

df[targets].info()

"""## Feature scaling"""

mm_scaler = MinMaxScaler()
df[features] = pd.DataFrame(mm_scaler.fit_transform(df[features]))

df.shape

"""## Targets analysis"""

### Cheking for Skew & Kurtosis
df_results = pd.DataFrame()
for target in targets:
    df_results.at[target, "skew"] = skew(df[target], nan_policy = "omit")
    df_results.at[target, "kurt"] = kurtosis(df[target], nan_policy = "omit")
df_results

"""### r1_iram1622"""

target = "r1_iram1622"
target_skew = skew(df[target], nan_policy = "omit")
target_kurtosis = kurtosis(df[target], nan_policy = "omit")
print("Skew:", target_skew, "   Kurtosis:", target_kurtosis)
print("DF Shape:", df.shape)
sns.set(rc={"figure.figsize": (8, 3)})
sns.histplot(data = df[target], bins = 60, color = 'darkblue', kde = True)
###
fig = plt.figure(figsize = (8, 3))
df[target].plot(kind = "box", vert = False)
plt.show()

print(df[df[target] > 27].shape[0])
idx_2_drop = df[df[target] > 27].index.tolist()
print(idx_2_drop)

df = df.drop(idx_2_drop, axis = 0)
df.shape

"""## Targets correlation"""

df1 = df[targets].copy()
df1.isna().sum()

df1 = df1.dropna()
df1.shape

target_corr = df1.corr(method = "pearson")[target].sort_values(ascending = False)
print(target_corr)

fig, ax = plt.subplots(figsize=(4, 3))
sns.heatmap(df1.corr(), vmin = 0, vmax = 1, cmap = "Blues")

"""## Cross validation: Strength Day 1
### Dummy, linear, and catboost regressor
"""

features.append(target)
df2 = df[features].dropna().copy()
df2.reset_index(drop = True, inplace = True)

features.remove(target)
X, y = df2[features], df2[target]
cv_folds = KFold(n_splits = 5, random_state = 12721, shuffle = True)

names = ["dummy", "linear", "catboost"]
regressors = [DummyRegressor(strategy = "mean"),
              LinearRegression(),
              CatBoostRegressor(loss_function = "MAE", verbose = False)]

for name, clf in zip(names, regressors):
    cv_scores = cross_val_score(clf, X, y, scoring = 'neg_mean_absolute_error', cv = cv_folds, n_jobs = -1)
    print(name, -cv_scores, 'Mean Squared Error: %.3f (%.3f)' % (mean(-cv_scores), std(cv_scores)))

"""## Cross validation: Strength Day 7
### Dummy, linear, and catboost regressor
"""

features.append('r1_iram1622')
features.append('r7_iram1622')
df3 = df[features].copy()
df3 = df3.dropna()
df3.reset_index(drop = True, inplace = True)
df3.head(3)

target = 'r7_iram1622'
features.remove(target)
X, y = df3[features], df3[target]
cv_folds = KFold(n_splits = 5, random_state = 12, shuffle = True)

names = ["dummy", "linear", "catboost"]
regressors = [DummyRegressor(strategy = "mean"),
              LinearRegression(),
              CatBoostRegressor(loss_function = "MAE", verbose = False)]

for name, clf in zip(names, regressors):
    cv_scores = cross_val_score(clf, X, y, scoring = 'neg_mean_absolute_error', cv = cv_folds, n_jobs = -1)
    print(name, -cv_scores, 'Mean Squared Error: %.3f (%.3f)' % (mean(-cv_scores), std(cv_scores)))

"""## Feature importance using catboost regressor"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 757)

cbr_model = CatBoostRegressor(loss_function = "MAE", verbose = False)
cbr_model.fit(X_train, y_train)

importances = pd.DataFrame(data={'Attribute': X_train.columns, 'Importance': cbr_model.feature_importances_})
importances = importances.sort_values(by = 'Importance', ascending = False)
fig, ax = plt.subplots(figsize=(8, 3))
plt.bar(x = importances['Attribute'], height = importances['Importance'], color = '#087E8B')
plt.title('Feature importances obtained from coefficients', size = 10)
plt.xticks(rotation = 'vertical')
plt.show()

